---
layout: post
title: Word2Vec的一些理解
---
<br><br>

上周在公众号里看到陈老师（陈利人）写的一篇word2vec的讲解: [如果看了此文还不懂Word2Vec，那是我太笨](http://mp.weixin.qq.com/s/nPqrQJTIvVq36rU7NRghSg)。
正好上次大神也做过一次word2vec的分享，觉得挺有意思，就看了这篇博文和其它一些资料，在此跟大家分享一下我个人的理解。

## 1. Word2Vec是什么？
------
![Tomas Mikolov](http://7xqyb5.com1.z0.glb.clouddn.com/tomas-mikolov.png)

*Word2Vec*是一组用来学习和生成“词的向量表示法”(词嵌入)的模型，是2013年由Google的Tomas Mikolov（后来去了FAIR）等人提出来的，包含了两种训练模型： CBOW（Continuous Bag－of－Words Model）和Skip－gram模型，现在Word2Vec是深度学习在NLP中应用的基础部件。

*Word2Vec*既不是指某个算法，也不是指深度学习，但是它所包含的两种模型 CBOW 和 skip-gram 的确是比较浅层的神经元网络。

## 2. 动机：为什么要有Word2Vec这样的模型工具？
------
图像和语音处理系统处理的总是丰富的、多维的数据集，这些数据集要么是单点原始像素灰度所表示成的图像向量数据，要么是功率谱密度系数表示成的语音向量数据。对于像物体识别或语音识别这类的任务，人们实现所需要的信息都可以从这些向量数据中获得。但是传统的NLP系统却把词当作离散的原子符号，比如 'cat'表示成 Id537 and 'dog' 表示成 Id143，这种表示法对不同个体之间的联系毫无体现。而实用词的向量表示法就能克服很多这些障碍。

向量空间模型（Vector space models）在连续的向量空间里表示词语，其中语义上相似的词会映射到临近的点上去。VSMs在NLP中有很长的历史，所有的方法都是基于“在相同语境中出现的词有相似的语义”。不同的方法又可以分为两类：count-based methods (e.g. Latent Semantic Analysis潜在语义分析), and predictive methods (e.g. neural probabilistic language models)
Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model). 都是基于context－target pair来做的。

Word2Vec就是第二种，计算效率非常高的预测类模型，直接从原文中学习词嵌入。

## 3. Word2Vec两种训练模型的原理
------
概括地讲，Word2Vec 的训练模型，是具有一个隐含层的神经元网络"shallow" neural models（如下图）。它的输入是词汇表向量，当看到一个训练样本时，对于样本中的每一个词，就把相应的在词汇表中出现的位置的值置为1，否则置为0。它的输出也是词汇表向量，对于训练样本的标签中的每一个词，就把相应的在词汇表中出现的位置的值置为1，否则置为0。那么，对所有的样本，训练这个神经元网络。收敛之后，将从输入层到隐含层的那些权重，作为每一个词汇表中的词的向量。比如，第一个词的向量是（w1,1 w1,2 w1,3 ... w1,m），m是表示向量的维度。所有虚框中的权重就是所有词的向量的值。有了每个词的有限维度的向量，就可以用到其它的应用中，因为它们就像图像，有了有限维度的统一意义的输入。
<img src="http://mmbiz.qpic.cn/mmbiz_jpg/uVhqWvaiaiaPP9FqiaSH0OWTSoURSibdiaQdfknWoPSUcWAu3aEc14oNDKhoahGfR617BX89SU9aSv9FAE5epIDd95g/640">

举例来说，“the quick brown fox jumped over the lazy dog”，我们先从这句话生成数据集：（语境，词）对，语境可以就是左右两个词，于是就有：
> ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...

CBOW模型是根据语境来预测中间的词，而skip-gram则相反，根据中间的词来预测语境的词，那样数据集就要变成下面这个样子：
> (quick, the), (quick, brown), (brown, quick), (brown, fox), ...

**这些就是训练集的输入输出对，然后表示成词汇表向量，用神经元网路的标准反向传播算法（Backpropagation，BP）计算出每个词到隐含层的每个维度的权重，从而得到每个词的向量。**

实践中，CBOW模型对小的数据集效果较好，skip-gram模型对大的数据集更有效。Mikolov在他的论文中推荐使用的也是skip-gram model with negative sampling (``SGNS``)。

## 4. Word2Vec的实现
------
开源实现的工具：Continuous Bag-of-Words (CBOW) and the Skip-gram model (SG). [Github word2vec](https://github.com/dav/word2vec)

Python版本的，[gensim](http://radimrehurek.com/gensim/models/word2vec.html)

Java/Scala版本的，[deeplearning4j](https://deeplearning4j.org/word2vec.html)

TensorFlow里包含了它的实现，[Tutorials](https://www.tensorflow.org/tutorials/word2vec)

## 5. 仍留有的疑惑
------
1） 中间的浅层神经元网络是怎么训练的？

2） 隐含层的向量的维度m有什么意义，怎么进行取舍，与词汇表向量的维度n的关系？
<br><br><br>
